\documentclass[a4paper,10pt]{article}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage[toc,page]{appendix}
\usepackage{graphicx}

%Includes "References" in the table of contents
\usepackage[nottoc]{tocbibind}
\usepackage{titling}
\usepackage{setspace}

\parskip .8ex

%\setlength{\droptitle}{-15em} %%%% Add this line if need more space. %%%%

%Begining of the document
\begin{document}

\title{CSCM38: Adv Topic - Artificial Intelligence and Cyber Security - Coursework 1}
\author{Andy Gray\\445348}
\date{10/11/2020}

\maketitle

\section{Introduction}
\label{sec:intro}
	We will be looking at some issues surrounding an advanced topic within natural language processing (NLP).  NLP is a form of artificial intelligence  (AI) that aims to as the automatic manipulation of natural language, like speech and text, by software \cite{nlp_definition}. However, human language is highly ambiguous, ever-changing and evolving. People are great at producing language and understanding language, allowing them to be capable of expressing, perceiving, and interpreting very complex and nuanced meanings. At the same time, while we humans are great users of language, we are also not very good at formally understanding and explaining the rules that dictate our language \cite{goldberg2017neural}. So if the human language is difficult for humans, the process, therefore, can not be straight forward for computers either. However, some advancements of the years like [give examples of NLP processces]. 
	
	A Swiss linguistics professor in the 1900s, Ferdinand de Saussure, created the concept of "Language as a Science \cite{koerner2013ferdinand}." However, Professor Saussure, around 1911, offered three courses at the University of Geneva. At this university is where it got developed as an approach describing languages as "systems." Within the language, a sound represents a concept, a concept that shifts its meaning as the context changes \cite{nlp_history}. It was not until the 1950s where a British mathematician named Alan Turing wrote a paper, laying out a test for a "thinking" machine. In his paper, he said that if a machine could be part of a conversation, and it imitated a human so well that there were no noticeable differences. The machine could be considered capable of thinking \cite{turing2004can}. However, the Hodgkin-Huxley model showed how the brain uses neurons in forming an electrical network. These events helped inspire the idea of AI, NLP, and the evolution of computers \cite{nlp_history}.
	
	In 1957 previous linguistic concepts got revolutionised, concluding that for a computer to understand a language, the sentence structure would have to be changed \cite{chomsky2002syntactic}. These revolutions created the style of grammar called Phase-Structure Grammar, which methodically translated natural language sentences into a format that is usable by computers \cite{nlp_history}. 
	
	In the 1960s is when research into AI and NLP stopped due to the technology not being where it needed to be and costing more than hiring people to translate \cite{nlp_history}. It took until the 1980s for NLP and AI \cite{what_ai} research to resume after the failed expectations in the earlier years \cite{nlp_history}. However, it was not until the 1990s, where statistical models for NLP popularity started to grow. The pure statistics NLP methods have become remarkably valuable in keeping pace with the huge amounts of online text \cite{nlp_history}. N-Grams have become useful, recognising and tracking clumps of linguistic data, numerically \cite{n_grams}. In 1997, LSTM and recurrent neural net (RNN) models \cite{nlp_rnn} were introduced and found their niche in 2007 for voice and text processing. Currently, neural net models are considered the cutting edge of research and development in the NLP's understanding of text and speech generation \cite{nlp_history}.
	
	
	%[Explain/ overview of how NN have become good at NLP]
	
	[Project plan] \\With the introduction of Artificial Neural Networks (ANN) and the growth with popularity with frameworks like TensorFlow and PyTorch, we will be looking at how using a deep learning method of NLP might perform compared to a more traditional non-NN version.
	
	[overview of what to expect in assignment]\\
	First, we will look into the related work, covering what NLP is and some of the more traditional approaches, then we will look at the more recent advancements of NLP and look into how they work. Additionally, we will be looking at what advantages they are claiming to have over the traditional methods and each other.

\section{Related Work}
\label{sec:related_work}

\subsection{Traditional ML Approaches}

The most popular supervised NLP machine learning algorithms are \cite{ml_nlp}:
Support Vector Machines
Bayesian Networks
Maximum Entropy
Conditional Random Field

\subsection{Recurrent NN (RNN) for NLP}
\subsection{BERT for NLP}
\subsection{LSTM for NLP}



\section{Project Plan}
\label{sec:project_plan}


\medskip
\newpage
\begin{appendices}
	
	
\end{appendices}

\newpage

%Sets the bibliography style to UNSRT and imports the 
%bibliography file "samples.bib".
\bibliographystyle{acm}
\bibliography{samples}

\end{document}