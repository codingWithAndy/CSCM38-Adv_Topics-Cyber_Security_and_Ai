{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow                              import keras\n",
    "from tensorflow.keras                        import layers, Sequential\n",
    "from tensorflow.keras.preprocessing.text     import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics         import confusion_matrix\n",
    "\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/c/nlp-getting-started: NLP Disaster Tweets dataset\n",
    "df = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3271\n4342\n"
     ]
    }
   ],
   "source": [
    "print((df.target == 1).sum()) # Disaster\n",
    "print((df.target == 0).sum()) # No Disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# Preprocessing\n",
    "import re\n",
    "import string\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    return url.sub(r\"\", text)\n",
    "\n",
    "# https://stackoverflow.com/questions/34293875/how-to-remove-punctuation-marks-from-a-string-in-python-3-x-using-translate/34294022\n",
    "def remove_punct(text):\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "@bbcmtd Wholesale Markets ablaze http://t.co/lHYXEOHY6C\nt\n@bbcmtd Wholesale Markets ablaze \n"
     ]
    }
   ],
   "source": [
    "pattern = re.compile(r\"https?://(\\S+|www)\\.\\S+\")\n",
    "\n",
    "for t in df.text:\n",
    "    matches = pattern.findall(t)\n",
    "    for match in matches:\n",
    "        print(t)\n",
    "        print(match)\n",
    "        print(pattern.sub(r\"\", t))\n",
    "    if len(matches) > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df.text.map(remove_URL)\n",
    "df[\"text\"] = df.text.map(remove_punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Andy/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "# Stop Words: A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine\n",
    "# has been programmed to ignore, both when indexing entries for searching and when retrieving them \n",
    "# as the result of a search query.\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "\n",
    "# https://stackoverflow.com/questions/5486337/how-to-remove-stop-words-using-nltk-or-python\n",
    "def remove_stopwords(text):\n",
    "    filtered_words = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df.text.map(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0            deeds reason earthquake may allah forgive us\n",
       "1                   forest fire near la ronge sask canada\n",
       "2       residents asked shelter place notified officer...\n",
       "3       13000 people receive wildfires evacuation orde...\n",
       "4       got sent photo ruby alaska smoke wildfires pou...\n",
       "                              ...                        \n",
       "7608    two giant cranes holding bridge collapse nearb...\n",
       "7609    ariaahrary thetawniest control wild fires cali...\n",
       "7610                      m194 0104 utc5km volcano hawaii\n",
       "7611    police investigating ebike collided car little...\n",
       "7612    latest homes razed northern california wildfir...\n",
       "Name: text, Length: 7613, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "df.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Count unique words\n",
    "def counter_word(text_col):\n",
    "    count = Counter()\n",
    "    for text in text_col.values:\n",
    "        for word in text.split():\n",
    "            count[word] += 1\n",
    "    return count\n",
    "\n",
    "\n",
    "counter = counter_word(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "17971"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "len(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[('like', 345), ('im', 299), ('amp', 298), ('fire', 250), ('get', 229)]"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "counter.most_common(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "17971\n"
     ]
    }
   ],
   "source": [
    "num_unique_words = len(counter)\n",
    "print(num_unique_words)"
   ]
  },
  {
   "source": [
    "### Creating X, y data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.text.to_numpy()\n",
    "y = df.target.to_numpy()"
   ]
  },
  {
   "source": [
    "### Splitting the data into Training and testing"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a test train set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and validation set\n",
    "train_size = int(X_train.shape[0] * 0.8)\n",
    "\n",
    "train_df = X_train[:train_size]\n",
    "val_df   = X_train[train_size:]\n",
    "\n",
    "#print(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text and labels\n",
    "train_sentences = train_df\n",
    "train_labels    = y_train[:train_size]\n",
    "val_sentences   = val_df\n",
    "val_labels      = y_train[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_sentences.shape, val_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = X_test\n",
    "test_labels    = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2513,)"
      ]
     },
     "metadata": {},
     "execution_count": 158
    }
   ],
   "source": [
    "test_sentences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "# vectorize a text corpus by turning each text into a sequence of integers\n",
    "tokenizer = Tokenizer(num_words=num_unique_words)\n",
    "tokenizer.fit_on_texts(train_sentences) # fit only to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each word has unique index\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "test_sequences  = tokenizer.texts_to_sequences(test_sentences)\n",
    "val_sequences   = tokenizer.texts_to_sequences(val_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_sentences[10:15])\n",
    "#print(train_sequences[10:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sequences to have the same length\n",
    "# Max number of words in a sequence\n",
    "max_length = 20\n",
    "\n",
    "train_padded = pad_sequences(train_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "val_padded   = pad_sequences(val_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_padded = pad_sequences(test_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_padded.shape, test_padded.shape, val_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_padded[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_sentences[10])\n",
    "#print(train_sequences[10])\n",
    "#print(train_padded[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check reversing the indices\n",
    "\n",
    "# flip (key, value)\n",
    "reverse_word_index = dict([(idx, word) for (word, idx) in word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reverse_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(sequence):\n",
    "    return \" \".join([reverse_word_index.get(idx, \"?\") for idx in sequence])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_text = decode(train_sequences[10])\n",
    "\n",
    "#print(train_sequences[10])\n",
    "#print(decoded_text)"
   ]
  },
  {
   "source": [
    "## Shared Metrics"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss    = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "optim   = keras.optimizers.Adam(lr=0.001)\n",
    "metrics = [\"accuracy\"]"
   ]
  },
  {
   "source": [
    "## Creating the LSTM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_cells = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_16\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_16 (Embedding)     (None, 20, 32)            575072    \n_________________________________________________________________\nlstm_32 (LSTM)               (None, 20, 32)            8320      \n_________________________________________________________________\nlstm_33 (LSTM)               (None, 20, 32)            8320      \n_________________________________________________________________\nlstm_34 (LSTM)               (None, 20, 32)            8320      \n_________________________________________________________________\nlstm_35 (LSTM)               (None, 32)                8320      \n_________________________________________________________________\ndense_16 (Dense)             (None, 1)                 33        \n=================================================================\nTotal params: 608,385\nTrainable params: 608,385\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "LSTM_model = Sequential()\n",
    "LSTM_model.add(layers.Embedding(num_unique_words, 32,input_length=max_length))\n",
    "\n",
    "# The layer will take as input an integer matrix of size (batch, input_length),\n",
    "# and the largest integer (i.e. word index) in the input should be no larger than num_words (vocabulary size).\n",
    "# Now model.output_shape is (None, input_length, 32), where `None` is the batch dimension.\n",
    "\n",
    "LSTM_model.add(layers.LSTM(no_of_cells, dropout=0.1, return_sequences=True))\n",
    "LSTM_model.add(layers.LSTM(no_of_cells, dropout=0.1, return_sequences=True))\n",
    "LSTM_model.add(layers.LSTM(no_of_cells, dropout=0.1, return_sequences=True))\n",
    "LSTM_model.add(layers.LSTM(no_of_cells, dropout=0.1))\n",
    "LSTM_model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "LSTM_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_model.compile(loss=loss, optimizer=optim, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "128/128 [==============================] - 5s 40ms/step - loss: 0.5447 - accuracy: 0.7267 - val_loss: 0.4845 - val_accuracy: 0.7882\n",
      "Epoch 2/20\n",
      "128/128 [==============================] - 4s 32ms/step - loss: 0.2311 - accuracy: 0.9145 - val_loss: 0.5797 - val_accuracy: 0.7833\n",
      "Epoch 3/20\n",
      "128/128 [==============================] - 4s 32ms/step - loss: 0.1232 - accuracy: 0.9613 - val_loss: 0.6703 - val_accuracy: 0.7676\n",
      "Epoch 4/20\n",
      "128/128 [==============================] - 4s 30ms/step - loss: 0.0754 - accuracy: 0.9752 - val_loss: 0.8655 - val_accuracy: 0.7559\n",
      "Epoch 5/20\n",
      "128/128 [==============================] - 4s 30ms/step - loss: 0.0554 - accuracy: 0.9816 - val_loss: 1.0503 - val_accuracy: 0.7667\n",
      "Epoch 6/20\n",
      "128/128 [==============================] - 4s 29ms/step - loss: 0.0483 - accuracy: 0.9811 - val_loss: 0.9578 - val_accuracy: 0.7824\n",
      "Epoch 7/20\n",
      "128/128 [==============================] - 4s 30ms/step - loss: 0.0435 - accuracy: 0.9838 - val_loss: 1.1931 - val_accuracy: 0.7471\n",
      "Epoch 8/20\n",
      "128/128 [==============================] - 4s 29ms/step - loss: 0.0394 - accuracy: 0.9858 - val_loss: 1.2360 - val_accuracy: 0.7696\n",
      "Epoch 9/20\n",
      "128/128 [==============================] - 4s 29ms/step - loss: 0.0386 - accuracy: 0.9855 - val_loss: 1.1185 - val_accuracy: 0.7608\n",
      "Epoch 10/20\n",
      "128/128 [==============================] - 4s 29ms/step - loss: 0.0334 - accuracy: 0.9850 - val_loss: 1.2735 - val_accuracy: 0.7667\n",
      "Epoch 11/20\n",
      "128/128 [==============================] - 4s 29ms/step - loss: 0.0317 - accuracy: 0.9877 - val_loss: 1.3148 - val_accuracy: 0.7716\n",
      "Epoch 12/20\n",
      "128/128 [==============================] - 4s 29ms/step - loss: 0.0299 - accuracy: 0.9870 - val_loss: 1.3960 - val_accuracy: 0.7608\n",
      "Epoch 13/20\n",
      "128/128 [==============================] - 4s 30ms/step - loss: 0.0300 - accuracy: 0.9870 - val_loss: 1.2509 - val_accuracy: 0.7725\n",
      "Epoch 14/20\n",
      "128/128 [==============================] - 4s 30ms/step - loss: 0.0275 - accuracy: 0.9860 - val_loss: 1.4138 - val_accuracy: 0.7676\n",
      "Epoch 15/20\n",
      "128/128 [==============================] - 4s 30ms/step - loss: 0.0285 - accuracy: 0.9870 - val_loss: 1.3619 - val_accuracy: 0.7667\n",
      "Epoch 16/20\n",
      "128/128 [==============================] - 4s 31ms/step - loss: 0.0263 - accuracy: 0.9855 - val_loss: 1.6394 - val_accuracy: 0.7559\n",
      "Epoch 17/20\n",
      "128/128 [==============================] - 4s 30ms/step - loss: 0.0274 - accuracy: 0.9860 - val_loss: 1.5588 - val_accuracy: 0.7627\n",
      "Epoch 18/20\n",
      "128/128 [==============================] - 4s 30ms/step - loss: 0.0234 - accuracy: 0.9877 - val_loss: 1.6490 - val_accuracy: 0.7667\n",
      "Epoch 19/20\n",
      "128/128 [==============================] - 4s 30ms/step - loss: 0.0241 - accuracy: 0.9882 - val_loss: 1.6716 - val_accuracy: 0.7608\n",
      "Epoch 20/20\n",
      "128/128 [==============================] - 4s 31ms/step - loss: 0.0224 - accuracy: 0.9873 - val_loss: 1.6034 - val_accuracy: 0.7706\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "LSTM_model.fit(train_padded, train_labels, epochs=20, validation_data=(val_padded, val_labels), verbose=1)\n",
    "\n",
    "running_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The training took: 83.22 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(\"The training took: {:.2f} seconds.\".format(running_time))"
   ]
  },
  {
   "source": [
    "# Testing LSTM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictting with the training data\n",
    "LSTM_predictions = LSTM_model.predict(test_padded)\n",
    "LSTM_predictions = [1 if p > 0.5 else 0 for p in LSTM_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(test_sentences[10:20])\n",
    "\n",
    "#print(test_labels[10:20])\n",
    "#print(LSTM_predictions[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1216,  335],\n",
       "       [ 230,  732]])"
      ]
     },
     "metadata": {},
     "execution_count": 211
    }
   ],
   "source": [
    "confusion_matrix(LSTM_predictions, test_labels)"
   ]
  },
  {
   "source": [
    "## Creating the GRU"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential_17\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_17 (Embedding)     (None, 20, 32)            575072    \n_________________________________________________________________\ngru_24 (GRU)                 (None, 20, 32)            6336      \n_________________________________________________________________\ngru_25 (GRU)                 (None, 20, 32)            6336      \n_________________________________________________________________\ngru_26 (GRU)                 (None, 32)                6336      \n_________________________________________________________________\ndense_17 (Dense)             (None, 1)                 33        \n=================================================================\nTotal params: 594,113\nTrainable params: 594,113\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "GRU_model = Sequential()\n",
    "GRU_model.add(layers.Embedding(num_unique_words, 32, input_length=max_length))\n",
    "\n",
    "GRU_model.add(layers.GRU(no_of_cells, dropout=0.1, return_sequences=True))\n",
    "GRU_model.add(layers.GRU(no_of_cells, dropout=0.1, return_sequences=True))\n",
    "GRU_model.add(layers.GRU(no_of_cells, dropout=0.1))\n",
    "GRU_model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "GRU_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRU_model.compile(loss=loss, optimizer=optim, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/20\n",
      "128/128 [==============================] - 4s 32ms/step - loss: 0.6202 - accuracy: 0.6483 - val_loss: 0.5010 - val_accuracy: 0.7725\n",
      "Epoch 2/20\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 0.3093 - accuracy: 0.8772 - val_loss: 0.5433 - val_accuracy: 0.7559\n",
      "Epoch 3/20\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 0.1540 - accuracy: 0.9478 - val_loss: 0.5922 - val_accuracy: 0.7627\n",
      "Epoch 4/20\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 0.0891 - accuracy: 0.9701 - val_loss: 0.7918 - val_accuracy: 0.7676\n",
      "Epoch 5/20\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 0.0640 - accuracy: 0.9792 - val_loss: 0.7025 - val_accuracy: 0.7559\n",
      "Epoch 6/20\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 0.0518 - accuracy: 0.9828 - val_loss: 0.8953 - val_accuracy: 0.7725\n",
      "Epoch 7/20\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 0.0390 - accuracy: 0.9850 - val_loss: 1.0629 - val_accuracy: 0.7608\n",
      "Epoch 8/20\n",
      "128/128 [==============================] - 3s 26ms/step - loss: 0.0339 - accuracy: 0.9875 - val_loss: 1.1974 - val_accuracy: 0.7667\n",
      "Epoch 9/20\n",
      "128/128 [==============================] - 4s 28ms/step - loss: 0.0270 - accuracy: 0.9865 - val_loss: 1.2481 - val_accuracy: 0.7647\n",
      "Epoch 10/20\n",
      "128/128 [==============================] - 4s 28ms/step - loss: 0.0230 - accuracy: 0.9860 - val_loss: 1.2557 - val_accuracy: 0.7578\n",
      "Epoch 11/20\n",
      "128/128 [==============================] - 3s 25ms/step - loss: 0.0228 - accuracy: 0.9870 - val_loss: 1.5393 - val_accuracy: 0.7627\n",
      "Epoch 12/20\n",
      "128/128 [==============================] - 3s 26ms/step - loss: 0.0227 - accuracy: 0.9868 - val_loss: 1.4246 - val_accuracy: 0.7578\n",
      "Epoch 13/20\n",
      "128/128 [==============================] - 3s 26ms/step - loss: 0.0258 - accuracy: 0.9870 - val_loss: 1.0945 - val_accuracy: 0.7735\n",
      "Epoch 14/20\n",
      "128/128 [==============================] - 3s 27ms/step - loss: 0.0235 - accuracy: 0.9880 - val_loss: 1.3459 - val_accuracy: 0.7520\n",
      "Epoch 15/20\n",
      "128/128 [==============================] - 3s 25ms/step - loss: 0.0218 - accuracy: 0.9882 - val_loss: 1.2119 - val_accuracy: 0.7618\n",
      "Epoch 16/20\n",
      "128/128 [==============================] - 3s 26ms/step - loss: 0.0205 - accuracy: 0.9877 - val_loss: 1.4839 - val_accuracy: 0.7657\n",
      "Epoch 17/20\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 0.0201 - accuracy: 0.9880 - val_loss: 1.4483 - val_accuracy: 0.7520\n",
      "Epoch 18/20\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 0.0194 - accuracy: 0.9875 - val_loss: 1.5755 - val_accuracy: 0.7608\n",
      "Epoch 19/20\n",
      "128/128 [==============================] - 3s 25ms/step - loss: 0.0190 - accuracy: 0.9873 - val_loss: 1.6488 - val_accuracy: 0.7510\n",
      "Epoch 20/20\n",
      "128/128 [==============================] - 3s 26ms/step - loss: 0.0193 - accuracy: 0.9880 - val_loss: 1.6071 - val_accuracy: 0.7657\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "GRU_model.fit(train_padded, train_labels, epochs=20, validation_data=(val_padded, val_labels), verbose=1)\n",
    "running_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The training took: 69.52 seconds.\n"
     ]
    }
   ],
   "source": [
    "print(\"The training took: {:.2f} seconds.\".format(running_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRU_predictions = GRU_model.predict(test_padded)\n",
    "GRU_predictions = [1 if p > 0.5 else 0 for p in GRU_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(train_sentences[10:20])\n",
    "\n",
    "#print(test_labels[10:20])\n",
    "#print(GRU_predictions[10:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1198,  326],\n",
       "       [ 248,  741]])"
      ]
     },
     "metadata": {},
     "execution_count": 218
    }
   ],
   "source": [
    "confusion_matrix(GRU_predictions, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "4853882b697e6b0431e01bed671aeb3a0e9ddaa05d67de9c41ea02ac68e863a0"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}